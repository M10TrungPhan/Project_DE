{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_test = spark \\\n",
    "            .read \\\n",
    "            .options(header=True) \\\n",
    "            .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark \\\n",
    "            .read \\\n",
    "            .options(header=True) \\\n",
    "            .schema(schema) \\\n",
    "            .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897493"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw \\\n",
    "    .repartition(6) \\\n",
    "    .write \\\n",
    "    .parquet('fhv_tripdata_2019-10', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark \\\n",
    "                .read \\\n",
    "                .parquet('fhv_tripdata_2019-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col \n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_interval_time(duration):\n",
    "    if duration is None:\n",
    "        return None\n",
    "    return duration / timedelta(hours=1)\n",
    "convert_interval_time_udf = F.udf(convert_interval_time, returnType=types.TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_interval_time((datetime(year=2019, day=15, month=10) - datetime(year=2019, day=11, month=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing = df_parquet \\\n",
    "                .withColumn('pickup_date', F.to_date(df_parquet.pickup_datetime)) \\\n",
    "                .withColumn('dropoff_date', F.to_date(df_parquet.dropoff_datetime)) \\\n",
    "                .withColumn('length_trips', convert_interval_time_udf(df_parquet.dropoff_datetime - df_parquet.pickup_datetime))\n",
    "                # .withColumn('length_trips_hours', convert_interval_time_udf(df_parquet.length_trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing = df_parquet \\\n",
    "                .withColumn('pickup_date', F.to_date(df_parquet.pickup_datetime)) \\\n",
    "                .withColumn('dropoff_date', F.to_date(df_parquet.dropoff_datetime)) \\\n",
    "                .withColumn('length_trips', df_parquet.dropoff_datetime - df_parquet.pickup_datetime)\n",
    "                # .withColumn('length_trips_hours', convert_interval_time_udf(df_parquet.length_trips))\n",
    "df_processing = df_processing \\\n",
    "                .withColumn('hours_trips', convert_interval_time_udf(col('length_trips')))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+------------+--------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|pickup_date|dropoff_date|        length_trips|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+------------+--------------------+\n",
      "|     B00889         |2019-10-01 15:38:29|2019-10-01 15:56:54|         129|          92|   null|       B00889         | 2019-10-01|  2019-10-01|INTERVAL '0 00:18...|\n",
      "|              B01239|2019-10-02 10:17:37|2019-10-02 10:33:46|         264|         241|   null|                B01239| 2019-10-02|  2019-10-02|INTERVAL '0 00:16...|\n",
      "|              B01745|2019-10-01 14:07:24|2019-10-01 14:19:02|         264|         215|   null|                B01745| 2019-10-01|  2019-10-01|INTERVAL '0 00:11...|\n",
      "|              B00256|2019-10-02 13:05:34|2019-10-02 13:54:04|         264|         264|   null|                B00256| 2019-10-02|  2019-10-02|INTERVAL '0 00:48...|\n",
      "|              B03060|2019-10-01 13:33:31|2019-10-01 13:49:13|         264|         155|   null|                B02875| 2019-10-01|  2019-10-01|INTERVAL '0 00:15...|\n",
      "|              B02120|2019-10-02 16:00:00|2019-10-02 16:53:00|         264|         264|   null|                B02120| 2019-10-02|  2019-10-02|INTERVAL '0 00:53...|\n",
      "|              B02107|2019-10-02 08:39:54|2019-10-02 08:52:23|         264|         168|   null|                B02107| 2019-10-02|  2019-10-02|INTERVAL '0 00:12...|\n",
      "|              B02715|2019-10-02 15:12:04|2019-10-02 15:55:52|         265|         161|   null|                B02765| 2019-10-02|  2019-10-02|INTERVAL '0 00:43...|\n",
      "|              B01346|2019-10-01 09:04:00|2019-10-01 09:20:11|         264|         228|   null|                B01346| 2019-10-01|  2019-10-01|INTERVAL '0 00:16...|\n",
      "|              B02550|2019-10-02 10:14:37|2019-10-02 10:43:45|         264|         265|   null|                B02550| 2019-10-02|  2019-10-02|INTERVAL '0 00:29...|\n",
      "|              B01239|2019-10-02 12:57:48|2019-10-02 13:16:54|         264|         242|   null|                B01239| 2019-10-02|  2019-10-02|INTERVAL '0 00:19...|\n",
      "|              B00937|2019-10-02 04:27:46|2019-10-02 04:39:52|         264|          94|   null|                B00937| 2019-10-02|  2019-10-02|INTERVAL '0 00:12...|\n",
      "|              B02429|2019-10-02 13:22:54|2019-10-02 15:30:58|         264|         264|   null|                B02429| 2019-10-02|  2019-10-02|INTERVAL '0 02:08...|\n",
      "|              B02590|2019-10-01 08:00:00|2019-10-01 08:11:00|         264|         264|   null|                B02590| 2019-10-01|  2019-10-01|INTERVAL '0 00:11...|\n",
      "|              B02735|2019-10-01 15:52:50|2019-10-01 16:21:39|         264|         212|   null|                B02875| 2019-10-01|  2019-10-01|INTERVAL '0 00:28...|\n",
      "|              B01907|2019-10-02 09:06:18|2019-10-02 11:05:56|         264|         265|   null|                B02682| 2019-10-02|  2019-10-02|INTERVAL '0 01:59...|\n",
      "|              B02784|2019-10-02 11:43:52|2019-10-02 12:27:45|         165|         170|   null|                  null| 2019-10-02|  2019-10-02|INTERVAL '0 00:43...|\n",
      "|              B02101|2019-10-01 10:04:36|2019-10-01 11:19:53|         264|         265|   null|                B02101| 2019-10-01|  2019-10-01|INTERVAL '0 01:15...|\n",
      "|              B02826|2019-10-01 15:52:44|2019-10-01 15:56:43|         264|          37|   null|                B02826| 2019-10-01|  2019-10-01|INTERVAL '0 00:03...|\n",
      "|              B00856|2019-10-01 15:17:34|2019-10-01 15:21:54|         264|         222|   null|                B00856| 2019-10-01|  2019-10-01|INTERVAL '0 00:04...|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_processing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ref = datetime(year=2019, day=15, month=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processing.filter(df_processing.pickup_date == date_ref).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 653, in mapper\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 653, in <genexpr>\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 101, in <lambda>\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 273, in toInternal\n    calendar.timegm(dt.utctimetuple()) if dt.tzinfo else time.mktime(dt.timetuple())\nAttributeError: 'float' object has no attribute 'tzinfo'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\envs\\dev\\lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\envs\\dev\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\envs\\dev\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 653, in mapper\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 653, in <genexpr>\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 101, in <lambda>\n  File \"C:\\tools\\spark-3.4.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 273, in toInternal\n    calendar.timegm(dt.utctimetuple()) if dt.tzinfo else time.mktime(dt.timetuple())\nAttributeError: 'float' object has no attribute 'tzinfo'\n"
     ]
    }
   ],
   "source": [
    "df_processing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
