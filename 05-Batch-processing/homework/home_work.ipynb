{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_test = spark \\\n",
    "            .read \\\n",
    "            .options(header=True) \\\n",
    "            .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = spark \\\n",
    "#             .read \\\n",
    "#             .options(header=True) \\\n",
    "#             .schema(schema) \\\n",
    "#             .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897493"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw \\\n",
    "    .repartition(6) \\\n",
    "    .write \\\n",
    "    .parquet('fhv_tripdata_2019-10', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark \\\n",
    "                .read \\\n",
    "                .parquet('fhv_tripdata_2019-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|     B00889         |2019-10-01 15:38:29|2019-10-01 15:56:54|         129|          92|   null|       B00889         |\n",
      "|              B01239|2019-10-02 10:17:37|2019-10-02 10:33:46|         264|         241|   null|                B01239|\n",
      "|              B01745|2019-10-01 14:07:24|2019-10-01 14:19:02|         264|         215|   null|                B01745|\n",
      "|              B00256|2019-10-02 13:05:34|2019-10-02 13:54:04|         264|         264|   null|                B00256|\n",
      "|              B03060|2019-10-01 13:33:31|2019-10-01 13:49:13|         264|         155|   null|                B02875|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col \n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2019-10-15|62610|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.groupby(F.to_date(\"pickup_datetime\").alias(\"date\")).count().filter(col('date')==\"2019-10-15\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing = df_parquet \\\n",
    "                .withColumn('length_trips_hours', \n",
    "                            F.round((df_parquet.dropoff_datetime.cast('long') - df_parquet.pickup_datetime.cast('long'))/3600,3)\n",
    "                            )\n",
    "                # .withColumn('length_trips_hours', convert_interval_time_udf(df_parquet.length_trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing.registerTempTable('fhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|max(length_trips_hours)|\n",
      "+-----------------------+\n",
      "|               631152.5|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT MAX(length_trips_hours) FROM fhv_data\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxizone = df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxizone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\anaconda3\\envs\\dev\\lib\\site-packages\\pyspark\\sql\\dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_taxizone.registerTempTable('taxi_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                zone|count(1)|\n",
      "+--------------------+--------+\n",
      "|                  NV| 1500773|\n",
      "|            Flushing|   12183|\n",
      "|     Jackson Heights|   10952|\n",
      "|                  NA|   10235|\n",
      "|         JFK Airport|    9307|\n",
      "|Saint George/New ...|    7680|\n",
      "|   LaGuardia Airport|    7603|\n",
      "|             Astoria|    7370|\n",
      "|              Corona|    7175|\n",
      "|        Midtown East|    6789|\n",
      "|        North Corona|    5964|\n",
      "|      Midtown Center|    5880|\n",
      "|            Elmhurst|    5763|\n",
      "|Times Sq/Theatre ...|    5451|\n",
      "|  Murray Hill-Queens|    5306|\n",
      "|         Old Astoria|    4493|\n",
      "|          Mount Hope|    3973|\n",
      "|            Steinway|    3955|\n",
      "|Van Nest/Morris Park|    3709|\n",
      "|    Sunset Park East|    3476|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT t.zone, count(1) from fhv_data as f inner join taxi_zone as t on f.PULocationID=t.LocationID\n",
    "          group by t.zone order by 2 desc\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\anaconda3\\envs\\dev\\lib\\site-packages\\pyspark\\sql\\dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_parquet.registerTempTable(\"fhv_data_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" Select count(*) from fhv_data_2 where cast(pickup_datetime as date) =\"2019-10-15\"\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------+\n",
      "|    pickup_datetime|   dropoff_datetime| hours|\n",
      "+-------------------+-------------------+------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|631152|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|631152|\n",
      "|2019-10-31 23:46:33|2029-11-01 00:13:00| 87672|\n",
      "|2019-10-01 21:43:42|2027-10-01 21:45:23| 70128|\n",
      "|2019-10-17 14:00:00|2020-10-18 00:00:00|  8794|\n",
      "|2019-10-26 21:26:00|2020-10-26 21:36:00|  8784|\n",
      "|2019-10-30 12:30:04|2019-12-30 13:02:08|  1464|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:54:33|  1056|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:21:11|  1056|\n",
      "|2019-10-01 07:21:12|2019-11-03 08:44:21|   793|\n",
      "|2019-10-01 13:47:17|2019-11-03 15:20:28|   793|\n",
      "|2019-10-01 13:41:00|2019-11-03 14:58:51|   793|\n",
      "|2019-10-01 08:56:38|2019-11-03 09:25:15|   792|\n",
      "|2019-10-01 16:28:23|2019-11-03 16:47:21|   792|\n",
      "|2019-10-01 08:18:17|2019-11-03 08:32:13|   792|\n",
      "|2019-10-01 15:07:47|2019-11-03 15:13:23|   792|\n",
      "|2019-10-01 14:18:49|2019-11-03 14:51:25|   792|\n",
      "|2019-10-01 14:40:26|2019-11-03 14:59:09|   792|\n",
      "|2019-10-01 07:53:39|2019-11-03 08:16:00|   792|\n",
      "|2019-10-01 14:24:33|2019-11-03 14:44:28|   792|\n",
      "+-------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" Select pickup_datetime,\n",
    "                     dropoff_datetime,\n",
    "                     timestampdiff(hour, pickup_datetime, dropoff_datetime) as hours\n",
    "            from fhv_data_2 \n",
    "            order by hours desc\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|             hours|\n",
      "+-------------------+-------------------+------------------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|          631152.5|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|          631152.5|\n",
      "|2019-10-31 23:46:33|2029-11-01 00:13:00| 87672.44083333333|\n",
      "|2019-10-01 21:43:42|2027-10-01 21:45:23| 70128.02805555555|\n",
      "|2019-10-17 14:00:00|2020-10-18 00:00:00|            8794.0|\n",
      "|2019-10-26 21:26:00|2020-10-26 21:36:00| 8784.166666666666|\n",
      "|2019-10-30 12:30:04|2019-12-30 13:02:08|1464.5344444444445|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:54:33|1056.8266666666666|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:21:11|1056.2705555555556|\n",
      "|2019-10-01 13:47:17|2019-11-03 15:20:28| 793.5530555555556|\n",
      "|2019-10-01 07:21:12|2019-11-03 08:44:21| 793.3858333333334|\n",
      "|2019-10-01 13:41:00|2019-11-03 14:58:51|          793.2975|\n",
      "|2019-10-01 18:43:20|2019-11-03 19:43:13| 792.9980555555555|\n",
      "|2019-10-01 18:43:46|2019-11-03 19:43:04| 792.9883333333333|\n",
      "|2019-10-01 07:07:09|2019-11-03 07:58:46| 792.8602777777778|\n",
      "|2019-10-01 14:49:28|2019-11-03 15:38:07| 792.8108333333333|\n",
      "|2019-10-01 05:36:30|2019-11-03 06:23:36|           792.785|\n",
      "|2019-10-01 15:02:55|2019-11-03 15:49:05| 792.7694444444444|\n",
      "|2019-10-01 06:08:01|2019-11-03 06:53:15| 792.7538888888889|\n",
      "|2019-10-01 06:41:17|2019-11-03 07:26:04| 792.7463888888889|\n",
      "+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" Select pickup_datetime,\n",
    "                     dropoff_datetime,\n",
    "                     (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime))/3600 as hours\n",
    "            from fhv_data_2 \n",
    "            order by hours desc\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
